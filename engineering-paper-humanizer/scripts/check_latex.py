#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""engineering-paper-humanizer LaTeX æ ¼å¼æ£€æŸ¥è„šæœ¬

æ‰«æ .tex æ–‡ä»¶ï¼Œæ£€æµ‹å¸¸è§çš„ AIGC æ®‹ç•™æ ¼å¼é—®é¢˜å’Œ LaTeX è§„èŒƒè¿è§„ã€‚
è¾“å‡ºç»“æ„åŒ–çš„é€è¡Œè¯Šæ–­ç»“æœï¼Œä¾› agent æˆ–äººå·¥å¿«é€Ÿå®šä½ä¿®å¤ã€‚

ç”¨æ³•:
    python3 scripts/check_latex.py <file.tex>
    python3 scripts/check_latex.py <file.tex> --section 3
    python3 scripts/check_latex.py <file.tex> --json
"""

import re
import sys
import json
import argparse
from pathlib import Path


# â”€â”€ è§„åˆ™å®šä¹‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

RULES = [
    # â”€â”€ å¼•ç”¨æ ¼å¼ â”€â”€
    # â”€â”€ åœ¨ RULES åˆ—è¡¨æœ«å°¾ã€æœ€åä¸€ä¸ª } ä¹‹åè¿½åŠ  â”€â”€
    # {
    #     "id": "AIGC-009",           # è§„åˆ™ IDï¼ŒæŒ‰ç±»åˆ«ç¼–å·é€’å¢
    #     "severity": "info",         # error / warning / info ä¸‰é€‰ä¸€
    #     "pattern": r'ä»è€Œå®ç°',      # Python æ­£åˆ™è¡¨è¾¾å¼
    #     "message": "æ£€æµ‹åˆ° AI æ•æ„Ÿè¯"ä»è€Œå®ç°"",
    #     "fix": "æ›¿æ¢ä¸º"ä»¥ä¿è¯"æˆ–"è¿«ä½¿"",
    # },
    {
        "id": "CITE-001",
        "severity": "error",
        "pattern": r'ã€‚\s*\\cite\{',
        "message": "\\cite{} å‡ºç°åœ¨å¥å·ä¹‹åï¼ˆåº”åœ¨å¥å·å†…ä¾§ï¼‰",
        "fix": "å°† \\cite{} ç§»åˆ°å¥å·å‰ï¼š...\\cite{xxx}ã€‚",
    },
    {
        "id": "CITE-002",
        "severity": "error",
        "pattern": r'ï¼Œ\s*\\cite\{',
        "message": "\\cite{} å‡ºç°åœ¨é€—å·ä¹‹åï¼ˆåº”åœ¨é€—å·å†…ä¾§ï¼‰",
        "fix": "å°† \\cite{} ç§»åˆ°é€—å·å‰ï¼š...\\cite{xxx}ï¼Œ",
    },
    {
        "id": "CITE-003",
        "severity": "error",
        "pattern": r'(?<=[^\s\\])\s+\\cite\{',
        "message": "\\cite{} å‰å­˜åœ¨ç©ºæ ¼",
        "fix": "åˆ é™¤ \\cite{} å‰çš„ç©ºæ ¼ï¼Œä½¿å…¶ç´§è´´è¢«å¼•æ–‡å­—",
    },
    # â”€â”€ æ ‡ç‚¹è§„èŒƒ â”€â”€
    {
        "id": "PUNCT-001",
        "severity": "warning",
        "pattern": r'(?<![\\])"[^"]*"',
        "message": "æ£€æµ‹åˆ°è‹±æ–‡åŒå¼•å·ï¼Œä¸­æ–‡è®ºæ–‡åº”ä½¿ç”¨ \u201c \u201d",
        "fix": "æ›¿æ¢ä¸ºä¸­æ–‡åŒå¼•å· \\u201c \\u201d",
    },
    {
        "id": "PUNCT-002",
        "severity": "warning",
        "pattern": r'â€”â€”',
        "message": "æ£€æµ‹åˆ°ä¸­æ–‡ç ´æŠ˜å·ç”¨äºé•¿è§£é‡Šï¼ˆå»ºè®®åŒ–å…¥æ­£æ–‡ï¼‰",
        "fix": "å°†ç ´æŠ˜å·è§£é‡Šå†…å®¹æ”¹å†™ä¸ºæ­£æ–‡ä»å¥",
    },
    {
        "id": "PUNCT-003",
        "severity": "warning",
        "pattern": r'[ï¼‰)]\s*[ï¼ˆ(]',
        "message": "æ£€æµ‹åˆ°è¿ç»­æ‹¬å·å †ç Œ",
        "fix": "åˆå¹¶æ‹¬å·å†…å®¹æˆ–å°†è§£é‡ŠåŒ–å…¥æ­£æ–‡",
    },
    # â”€â”€ LaTeX ä¿æŠ¤ â”€â”€
    {
        "id": "LATEX-001",
        "severity": "error",
        "pattern": r'(?<!\\)%(?!\\)',
        "message": "è£¸ç™¾åˆ†å· % æœªè½¬ä¹‰ï¼ˆä¼šå¯¼è‡´è¡Œå°¾æˆªæ–­ï¼‰",
        "fix": "æ”¹ä¸º \\%",
    },
    {
        "id": "LATEX-002",
        "severity": "error",
        "pattern": r'\\(?:cite|label|ref|includegraphics)\{[^}]*\n',
        "message": "LaTeX å‘½ä»¤èŠ±æ‹¬å·å†…éƒ¨å­˜åœ¨æ¢è¡Œ",
        "fix": "å°†å‘½ä»¤å‚æ•°åˆå¹¶åˆ°åŒä¸€è¡Œ",
    },
    # â”€â”€ AIGC ç—•è¿¹ â”€â”€
    {
        "id": "AIGC-001",
        "severity": "info",
        "pattern": r'æœ¬[ç« èŠ‚]å°†(?:é’ˆå¯¹|è¯¦ç»†|ä¸»è¦|é‡ç‚¹)',
        "message": "æ£€æµ‹åˆ°ç« èŠ‚é¢„å‘Šå¥å¼ï¼ˆAIGC é«˜é¢‘ç‰¹å¾ï¼‰",
        "fix": "åˆ é™¤é¢„å‘Šï¼Œç”¨å®¢è§‚ä¸»è¯­è‡ªç„¶è¿‡æ¸¡",
    },
    {
        "id": "AIGC-002",
        "severity": "info",
        "pattern": r'(?:å…·æœ‰|æ‹¥æœ‰)(?:ååˆ†|éå¸¸|æå…¶)?é‡è¦çš„(?:å·¥ç¨‹|ç†è®º|å®é™…|ç°å®)(?:åº”ç”¨)?(?:ä»·å€¼|æ„ä¹‰)',
        "message": "æ£€æµ‹åˆ°ç©ºæ³›ä»·å€¼å£°æ˜ï¼ˆAIGC é«˜é¢‘ç‰¹å¾ï¼‰",
        "fix": "åˆ é™¤æˆ–æ”¹ä¸ºå…·ä½“å­¦æœ¯è´¡çŒ®æè¿°",
    },
    {
        "id": "AIGC-003",
        "severity": "info",
        "pattern": r'(?:çªç ´|æ”»å…‹)äº†?.*?(?:æŠ€æœ¯ç“¶é¢ˆ|å…³é”®éš¾é¢˜)',
        "message": "æ£€æµ‹åˆ° AI æ•æ„Ÿè¯ç»„\u201cçªç ´æŠ€æœ¯ç“¶é¢ˆ\u201d",
        "fix": "æ›¿æ¢ä¸º\u201cå…‹æœ...æŠ€æœ¯éš¾ç‚¹\u201d",
    },
    {
        "id": "AIGC-004",
        "severity": "info",
        "pattern": r'(?:æä¾›|å¥ å®š)äº†?.*?(?:ç†è®º|æ•°æ®|æŠ€æœ¯)(?:æ”¯æ’‘|æ”¯æŒ|åŸºç¡€)',
        "message": "æ£€æµ‹åˆ° AI å‡åç»“å°¾\u201cæä¾›æ•°æ®æ”¯æ’‘\u201d",
        "fix": "åˆ é™¤å‡åå¥ï¼Œé™ˆè¿°å®ŒæŠ€æœ¯äº‹å®å³ç»“æŸæ®µè½",
    },
    {
        "id": "AIGC-005",
        "severity": "info",
        "pattern": r'(?:è™½ç„¶|å°½ç®¡).*?(?:å–å¾—äº†|å·²ç»|å·²æœ‰).*?(?:ä½†æ˜¯?|ç„¶è€Œ|ä¸è¿‡).*?(?:ä»ç„¶?|ä¾ç„¶|å°š)',
        "message": "æ£€æµ‹åˆ°\u201cè™½ç„¶...ä½†ä»...\u201dæ¨¡æ¿å¥ï¼ˆAIGC é«˜é¢‘æ¨¡å¼ï¼‰",
        "fix": "ç›´æ¥å®¢è§‚é™ˆè¿°åŒæ–¹ä¾§é‡ç‚¹",
    },
    {
        "id": "AIGC-006",
        "severity": "info",
        "pattern": r'åº”è¿è€Œç”Ÿ',
        "message": "æ£€æµ‹åˆ° AI æ•æ„Ÿè¯\u201cåº”è¿è€Œç”Ÿ\u201d",
        "fix": "ç›´æ¥åˆ é™¤",
    },
    {
        "id": "AIGC-007",
        "severity": "info",
        "pattern": r'åŒé‡å›°å¢ƒ',
        "message": "æ£€æµ‹åˆ° AI æ•æ„Ÿè¯\u201cåŒé‡å›°å¢ƒ\u201d",
        "fix": "æ›¿æ¢ä¸º\u201cåŒé‡é—®é¢˜\u201dæˆ–\u201cå±€é™æ€§\u201d",
    },
    {
        "id": "AIGC-008",
        "severity": "info",
        "pattern": r'æ—¨åœ¨(?:è§£å†³|æ¢è®¨|ç ”ç©¶)',
        "message": "æ£€æµ‹åˆ° AI æ•æ„Ÿè¯\u201cæ—¨åœ¨è§£å†³/æ¢è®¨\u201d",
        "fix": "æ›¿æ¢ä¸º\u201cè‡´åŠ›äºæ”¹å–„\u201dæˆ–\u201cæ·±å…¥åˆ†æäº†\u201d",
    },
]

# è¿æ¥è¯æ³›æ»¥æ£€æµ‹ï¼ˆç‹¬ç«‹ç»Ÿè®¡ï¼‰
CONNECTIVES = [
    'å› æ­¤', 'ç„¶è€Œ', 'å€¼å¾—æ³¨æ„çš„æ˜¯', 'æ­¤å¤–', 'ä¸æ­¤åŒæ—¶',
    'ç»¼ä¸Šæ‰€è¿°', 'ç”±æ­¤å¯è§', 'ä¸ä»…å¦‚æ­¤', 'æ›´é‡è¦çš„æ˜¯',
    'æ€»è€Œè¨€ä¹‹', 'æ¢è¨€ä¹‹', 'äº‹å®ä¸Š', 'æ˜¾è€Œæ˜“è§',
]


# â”€â”€ æ ¸å¿ƒé€»è¾‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ï¿½ï¿½ï¿½â”€â”€â”€â”€â”€

def is_in_math_env(line: str, match_start: int) -> bool:
    """ç²—ç•¥åˆ¤æ–­åŒ¹é…ä½ç½®æ˜¯å¦å¤„äºè¡Œå†…æ•°å­¦ç¯å¢ƒ $ $ å†…éƒ¨"""
    dollars = [m.start() for m in re.finditer(r'(?<!\\)\$', line)]
    depth = 0
    for d in dollars:
        if d >= match_start:
            break
        depth += 1
    return depth % 2 == 1


def is_in_block_math(lines: list[str], line_idx: int) -> bool:
    """åˆ¤æ–­å½“å‰è¡Œæ˜¯å¦å¤„äº equation/align ç­‰å—çº§æ•°å­¦ç¯å¢ƒå†…"""
    depth = 0
    math_envs = ('equation', 'align', 'gather', 'multline', 'eqnarray', 'math', 'displaymath')
    for i in range(line_idx + 1):
        for env in math_envs:
            depth += len(re.findall(rf'\\begin\{{{env}\*?\}}', lines[i]))
            depth -= len(re.findall(rf'\\end\{{{env}\*?\}}', lines[i]))
    return depth > 0


def check_file(filepath: str, section: int | None = None) -> list[dict]:
    """æ‰§è¡Œå…¨éƒ¨æ£€æŸ¥è§„åˆ™ï¼Œè¿”å›è¯Šæ–­åˆ—è¡¨"""
    path = Path(filepath)
    if not path.exists():
        print(f"Error: file not found: {filepath}", file=sys.stderr)
        sys.exit(1)

    text = path.read_text(encoding='utf-8')
    lines = text.splitlines()

    # å¦‚æœæŒ‡å®šäº† sectionï¼Œå®šä½èŒƒå›´
    start_line, end_line = 0, len(lines)
    if section is not None:
        sec_pattern = re.compile(rf'\\section\b')
        sec_positions = []
        for i, line in enumerate(lines):
            if sec_pattern.search(line):
                sec_positions.append(i)
        if section - 1 < len(sec_positions):
            start_line = sec_positions[section - 1]
            end_line = sec_positions[section] if section < len(sec_positions) else len(lines)

    diagnostics = []

    # é€è¡Œè§„åˆ™åŒ¹é…
    for i in range(start_line, end_line):
        line = lines[i]

        # è·³è¿‡æ³¨é‡Šè¡Œ
        stripped = line.lstrip()
        if stripped.startswith('%'):
            continue

        # è·³è¿‡å—çº§æ•°å­¦ç¯å¢ƒ
        if is_in_block_math(lines, i):
            # åªè·³è¿‡ AIGC/PUNCT ç±»è§„åˆ™ï¼ŒCITE/LATEX è§„åˆ™ä»ç„¶æ£€æŸ¥
            for rule in RULES:
                if rule["id"].startswith(("AIGC", "PUNCT")):
                    continue
                for m in re.finditer(rule["pattern"], line):
                    if not is_in_math_env(line, m.start()):
                        diagnostics.append({
                            "line": i + 1,
                            "column": m.start() + 1,
                            "rule": rule["id"],
                            "severity": rule["severity"],
                            "message": rule["message"],
                            "fix": rule["fix"],
                            "context": line.strip(),
                        })
        else:
            for rule in RULES:
                for m in re.finditer(rule["pattern"], line):
                    if not is_in_math_env(line, m.start()):
                        diagnostics.append({
                            "line": i + 1,
                            "column": m.start() + 1,
                            "rule": rule["id"],
                            "severity": rule["severity"],
                            "message": rule["message"],
                            "fix": rule["fix"],
                            "context": line.strip(),
                        })

    # è¿æ¥è¯æ³›æ»¥ç»Ÿè®¡
    connective_hits = []
    for i in range(start_line, end_line):
        line = lines[i]
        stripped = line.lstrip()
        if stripped.startswith('%'):
            continue
        if is_in_block_math(lines, i):
            continue
        for word in CONNECTIVES:
            if stripped.startswith(word):
                connective_hits.append({
                    "line": i + 1,
                    "column": 1,
                    "rule": "AIGC-CONN",
                    "severity": "info",
                    "message": f"æ®µ/å¥é¦–è¿æ¥è¯\u201c{word}\u201dï¼ˆè¿æ¥è¯æ³›æ»¥æ£€æµ‹ï¼‰",
                    "fix": "è¯„ä¼°æ˜¯å¦å¯åˆ é™¤ï¼Œç›®æ ‡å‰Šå‡ â‰¥ 50%",
                    "context": stripped[:60],
                })

    diagnostics.extend(connective_hits)

    # çªå‘æ€§ç²—è¯„ï¼ˆæ®µè½å†…å¥é•¿æ–¹å·®ï¼‰
    burstiness_warnings = check_burstiness(lines, start_line, end_line)
    diagnostics.extend(burstiness_warnings)

    # æŒ‰è¡Œå·æ’åº
    diagnostics.sort(key=lambda d: (d["line"], d["column"]))
    return diagnostics


def check_burstiness(lines: list[str], start: int, end: int) -> list[dict]:
    """ç²—ç•¥è¯„ä¼°æ®µè½å†…å¥å­é•¿åº¦çš„çªå‘æ€§ï¼ˆæ–¹å·®ï¼‰"""
    warnings = []
    para_start = None
    para_sentences: list[int] = []

    def _eval_para(p_start, sents):
        if len(sents) < 4:
            return None
        avg = sum(sents) / len(sents)
        variance = sum((s - avg) ** 2 for s in sents) / len(sents)
        # æ–¹å·®è¿‡ä½ â†’ å¥é•¿è¿‡äºå‡åŒ€ â†’ ä½çªå‘æ€§
        if avg > 0 and (variance ** 0.5) / avg < 0.25:
            return {
                "line": p_start + 1,
                "column": 1,
                "rule": "BURST-001",
                "severity": "warning",
                "message": f"è¯¥æ®µè½å¥é•¿æ–¹å·®è¿‡ä½ï¼ˆCV={((variance**0.5)/avg):.2f}ï¼‰ï¼Œç–‘ä¼¼ä½çªå‘æ€§",
                "fix": "æ’å…¥æçŸ­å¥ï¼ˆ3~5å­—ï¼‰æˆ–è¶…é•¿å‚æ•°å¥ï¼ˆ20+å­—ï¼‰ä»¥æå‡é¡¿æŒ«æ„Ÿ",
                "context": f"æ®µè½èµ·å§‹è¡Œï¼Œå« {len(sents)} å¥ï¼Œå¹³å‡å¥é•¿ {avg:.0f} å­—",
            }
        return None

    for i in range(start, end):
        line = lines[i].strip()
        # ç©ºè¡Œæˆ–ç¯å¢ƒè¾¹ç•Œè§†ä¸ºæ®µè½åˆ†éš”
        if not line or line.startswith('\\section') or line.startswith('\\subsection'):
            if para_sentences:
                w = _eval_para(para_start, para_sentences)
                if w:
                    warnings.append(w)
            para_start = None
            para_sentences = []
            continue

        if line.startswith('%') or line.startswith('\\begin') or line.startswith('\\end'):
            continue

        if para_start is None:
            para_start = i

        # æŒ‰ä¸­æ–‡å¥å·/é—®å·/å¹å·åˆ†å¥
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', line)
        for s in sentences:
            clean = re.sub(r'\\[a-zA-Z]+\{[^}]*\}', '', s)  # ç§»é™¤ LaTeX å‘½ä»¤
            clean = re.sub(r'[^\u4e00-\u9fff]', '', clean)   # åªç•™ä¸­æ–‡å­—
            if len(clean) >= 2:
                para_sentences.append(len(clean))

    # å¤„ç†æœ€åä¸€ä¸ªæ®µè½
    if para_sentences:
        w = _eval_para(para_start, para_sentences)
        if w:
            warnings.append(w)

    return warnings


# â”€â”€ è¾“å‡ºæ ¼å¼åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SEVERITY_ICONS = {
    "error": "ğŸ”´",
    "warning": "ğŸŸ¡",
    "info": "ğŸ”µ",
}


def format_text(diagnostics: list[dict], filepath: str) -> str:
    """æ ¼å¼åŒ–ä¸ºäººç±»å¯è¯»çš„æ–‡æœ¬æŠ¥å‘Š"""
    if not diagnostics:
        return f"âœ… {filepath}: æœªå‘ç°é—®é¢˜"

    counts = {"error": 0, "warning": 0, "info": 0}
    lines = []
    lines.append(f"{'='*60}")
    lines.append(f"  æ£€æŸ¥æ–‡ä»¶: {filepath}")
    lines.append(f"{'='*60}")

    for d in diagnostics:
        counts[d["severity"]] += 1
        icon = SEVERITY_ICONS[d["severity"]]
        lines.append(f"")
        lines.append(f"{icon} [{d['rule']}] L{d['line']}:{d['column']}  {d['message']}")
        lines.append(f"   ä¸Šä¸‹æ–‡: {d['context'][:80]}")
        lines.append(f"   ä¿®å¤å»ºè®®: {d['fix']}")

    lines.append(f"")
    lines.append(f"{'='*60}")
    lines.append(f"  æ±‡æ€»: {counts['error']} é”™è¯¯ | {counts['warning']} è­¦å‘Š | {counts['info']} æç¤º")
    lines.append(f"{'='*60}")

    return "\n".join(lines)


# â”€â”€ å…¥å£ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    parser = argparse.ArgumentParser(
        description="engineering-paper-humanizer LaTeX æ ¼å¼æ£€æŸ¥"
    )
    parser.add_argument("file", help="è¦æ£€æŸ¥çš„ .tex æ–‡ä»¶è·¯å¾„")
    parser.add_argument(
        "--section", type=int, default=None,
        help="åªæ£€æŸ¥æŒ‡å®šç« èŠ‚ç¼–å·ï¼ˆæŒ‰ \\section å‡ºç°é¡ºåºä» 1 è®¡æ•°ï¼‰"
    )
    parser.add_argument(
        "--json", action="store_true",
        help="è¾“å‡º JSON æ ¼å¼ï¼ˆä¾› agent è§£æï¼‰"
    )
    parser.add_argument(
        "--severity", default=None, choices=["error", "warning", "info"],
        help="åªæ˜¾ç¤ºæŒ‡å®šä¸¥é‡çº§åˆ«åŠä»¥ä¸Š"
    )
    args = parser.parse_args()

    diagnostics = check_file(args.file, args.section)

    # è¿‡æ»¤ä¸¥é‡çº§åˆ«
    if args.severity:
        levels = {"error": 3, "warning": 2, "info": 1}
        threshold = levels[args.severity]
        diagnostics = [d for d in diagnostics if levels[d["severity"]] >= threshold]

    if args.json:
        print(json.dumps(diagnostics, ensure_ascii=False, indent=2))
    else:
        print(format_text(diagnostics, args.file))


if __name__ == "__main__":
    main()